Blaze Execution
===============

 * [Blaze Function Use Cases](blazefunc-usecases.md)
 * [Blaze NumPy-like API](blaze-numpy-api.md)
 * [Elementwise Reductions](elwise-reduction-ufuncs.md)
 * [Blaze AIR](blaze-air.md)
 * [Deferred CKernel Interface](deferred-ckernel-interface.md)
 * [CKernel](ckernel-interface.md)

The blaze execution system takes blaze expressions,
which are built up by applying functions
over blaze arrays. Blaze arrays can describe data
or other (sub-)expression. The goal for the blaze
execution system is to determine a suitable evaluation
strategy compatible with the input data sources.
This may include assembling something like an SQL
query, composing existing compiled code, or
using Numba to JIT-compiling kernel implementations.

## Building An Expression DAG

In designing a system that builds up a DAG, and then
executes it, there is a clear tension between
overhead when working with small arrays,
and performance at scale. When doing small,
interactive computations, often the best thing is
to evaluate the expressions immediately, or at the
very least build and execute the graph with minimal
analysis of its structure. When doing computations
on large, distributed data, spending extra time
creating execution plans that minimize expensive
operations and using compiliation technology
to squeeze performance out of the local execution
is well worth it.

Expressions are simple nodes in a DAG, 
which are typed as they are created
to provide some error detection before evaluation.
Nodes are created through function application:

    # return a new blaze array with a deferred expression
    # c.expr is Add(a.expr, b.expr)
    # c.dshape is determined based on a multiple-dispatch
    #     lookup based on (a.dshape, b.dshape) in
    #     the blaze.add function.
    c = blaze.add(a, b)

High-level expressions may be sent over a network or
passed on to the execution engine to determine a
suitable evaluation plan. The evaluation plan may
include C ABI constructs to compose code from low level
libraries, JIT-compiled code, assembled queries, etc.

This document describes the design of the execution
system, how it goes from expressions to execution.

At the highest level we have blaze expressions, in
between we have an intermediate representation of
the expression along with execution details,
and at the lowest level we have a set of kernels linked
together by their arguments. This process works
as follows:

* operations over arrays are lazy and accumulate
  a directed acyclic graph.
* blaze.eval() starts the execution process,
  which involves conversion to AIR and
  executing the result


Blaze Expressions and Blaze Functions
-------------------------------------

 * [Blaze Function Use Cases](blazefunc-usecases.md)

Blaze expressions are generated by applying blaze
functions, which are the user facing
representation of functionality in blaze. These match
datashape type signatures to implementation kernels.
The `ufunc` and `gufunc` objects in NumPy can be
represented this way as particular forms of
these signatures.

```
# A ufunc type signature
(A... * int32, A... * int32) -> A... * int32
# A gufunc type signature
(A... * M * N * float64, A... * N * R * float64) -> A... * M * R * float64
```

We support open-ended extension through overloading
at the blaze function level.
There are two forms of overloading at play:

    - function overloading
    - kernel overloading

The former overloads a logical blaze function, for instance for typing
purposes. The second form allows kernel (implementation) overloading,
where kernels are associated logically with blaze functions for a
certain implementation kind. Implementation kinds include flypy, SQL,
ckernel, and so forth.


Blaze Expression Lowering to Blaze AIR
--------------------------------------

The highest level of the blaze execution system is taking the interface
provided to users of blaze, which includes blaze functions and data
descriptions from concrete input arrays, and lowers it to blaze AIR.


Blaze AIR JIT Compilation
-------------------------

 * [Blaze AIR Documentation](blaze-air.md)

Once we are in blaze AIR (Array Intermediate Representation), we apply
successive passes in a pipeline to reduce the expressions to kernels which
can be applied to their arguments. This is further described in the link
above.


The Deferred CKernel Interface
------------------------------

 * [Deferred CKernel Interface Documentation](deferred-ckernel-interface.md)

At the lowest level
are primitive C ABI interfaces designed to be interoperable across
any library boundaries, including between systems using different
standard libraries. These low level interfaces are used by the
higher level systems as JIT compilation targets, and as a way to
import implementation kernels from outside of blaze.

One fundamental aspect of both blaze and dynd is deferred execution.
For supporting deferred and cached execution at the low level, just one
small step above the ckernel interface, is the deferred ckernel.
This object provides a simple interface to building a ckernel whose
structure has already been determined up to the dynd type level, and
just needs dynd metadata and a kernel type (i.e. single or strided) to
build a ckernel.

One of the use cases driving the deferred dynamic kernel is to provide
individual kernels to blaze and dynd function dispatch. While many of
the functions provided by blaze will be JIT compiled LLVM bitcode, there
needs to also be a way to expose functions to blaze from external systems
which know little or nothing about blaze and dynd.


The CKernel Builder
-------------------

 * [CKernel Documentation](ckernel-interface.md)

The lowest level execution interface in blaze is the ckernel.
Any time an operation gets executed in blaze, it is first reduced
down into a ckernel, either via JIT compilation or assembling together
other ckernels, and then executed as a ckernel.

When constructing a ckernel from JIT compilation or deferred ckernels,
the `ckernel_builder` object is used. This is a small C struct with a
static buffer which can hold small ckernels, and API functions for
dynamically growing the ckernel buffer for larger ones.

At the ckernel level, all information about types and possible variations
about memory layout has been baked into the code and data that make
up the ckernel. All that is left is the ability to call the kernel function
and to free the resources associated with the ckernel. This means that
code using a ckernel can be quite simple, it just needs to know the ckernel's
function prototype, and have data pointers that it knows conforms to the
types baked into the ckernel, and it can execute it.
